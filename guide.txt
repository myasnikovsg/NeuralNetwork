Точка входа GUI\NeuralNetworkGUI.main()
Команды вводятся в командную строку, и выполняются по нажатию на enter либо по клику на кнопку go.
Командная строка хранит последние запросы, к ним можно перейти по клавишам вниз и вверх.
Командная строка не чуствительна к регистру.
Все команды начинаются со слова, символы -- для, удобства чтения и должны буть опущены при вводе команды.
Для начала работы требуется либо создать новую feedforward neural network (FFNN) или загрузить существующую из файла

Для создания используется команда Create
 -- Create LayersCount [int : LayersCount] neuronCountInLayer activationFunction
	LayersCount - количество слоев
	neuronCountInLayer - количество нейронов в каждом слое 
	activationFunction - активационная функция [sigmoid, linear, tanh]
Пример: Create 3 2 3 1 sigmoid 
создаст сеть с 3 слоями. Во входном слое расположено 2 нейрона, в спрятанном - 3 нейрона, и выходном - 1. Активационной функцией каждого нейрона служит сигмоид.

Для загрузки используется команда Load
 -- Load pathToFFNNFile
	pathToFFNNFile - путь к файлу, содержащему описание сети
Пример: load res\1.ffnn - загрузка из res

Для сохранения используется команда Save
 -- Save pathToFFNNFile
	pathToFFNNFile - путь к файлу для сохранения сети
Пример: save res\1.ffnn - сохранение в res

Также можно воспользоваться командой PruneInc, создающей новую сеть для реализации алгоритма Incremental Pruning.
 -- PruneInc learnRate momentum maxError inputNeuronCount outputNeuronCount activationFunction inputPath idealPath
	learnRate - скорость обучения, от 0 до 1, double
	momentum - скорость реакции одной итерации на предыдущие, 0 до 1, double
	maxError - максимальная допустимая ошибка, double
	inputNeuronCount - количество нейронов во входном слое
	outputNeuronCount - количество нейронов в выходном слое
	activationFunction - активационная функция [sigmoid, linear, tanh]
	inputPath - путь к файлу с тестовыми данными
	idealPath - путь к файлу с идеальными результами для тестовых данных
Пример: PruneInc 0.7 0.8 0.001 2 1 sigmoid res\input.tst res\ideal.tst
	
Для изменения веса аксона и threshold'ов нейронов используется команда Change
1) -- Change t layer number threshold
	layer - номер слоя(считая с 0)
	number - номер нейрона в слое(считая с 0)
	threshold - новое значения порога для нейрона, double
Пример: Change t 0 1 2.15 - устанавливает значение порога нейрона во входном слое номер 1 в новое значение 2.15
2) -- Change w layer numberFrom numberTo weight 
	layer - номер слоя(считая с 0)
	numberFrom - номер нейрона в слое(считая с 0)
	numberTo - номер нейрона в следующем слое (считая с 0)
	weight - новый вес для аксона
Пример: Change w 1 0 2 2.15 - изменяет вес ребра(аксона), ведущего от нейрона в слое 1 номер 0 к нейрону в слое 2 номер 2 на значение 2.15
3) Также, по нажатию правой клавишей по графическому изображению нейрона появляется меню, выполняющее те же функции

Команда Reset устанавливает случайные значения для всех значений нейронной сети
 -- Reset min max
	min - минимальное принимаемое значение
	min - максимальное принимаемое значение
Пример: Reset -1 1 установит случайные значения для весов всех аксонов и все порогов в диапазоне (-1, 1)

Для просмотра RMS Error(Root Main Square Error) текущей сети используется команда Error
 -- Error inputPath idealPath
	inputPath - путь к файлу с тестовыми данными
	idealPath - путь к файлу с идеальными результами для тестовых данных
Пример: Error res\input.tst res\ideal.tst

Для изменения количества выводимых в графическом представлении сети чисел применяется команда Precision
 -- Precision fractionNumber
	fractionNumber - количество отображаемых после запятой знаков
Пример: Precision 5 - после применения данной команды числа вида 2.00 будут отбражаться как 2.00000

Для начала тренировки(обучения) сети по алгоритму Backpropagation используется одноименная команда
 -- Backpropagation learnRate momentum inputPath idealPath
	learnRate - скорость обучения, от 0 до 1, double
	momentum - скорость реакции одной итерации на предыдущие, 0 до 1, double
	inputPath - путь к файлу с тестовыми данными
	idealPath - путь к файлу с идеальными результами для тестовых данных
Пример: Backpropagation 0.7 0.8 res\input.tst res\ideal.tst (необходима сеть с 2 входными и одним выходным нейроном)

Для начала тренировки сети по алгоритму Genetic Algorithm используется команда Genetic
 -- Genetic populationSize mutationPercent percentToMate inputPath idealPath
	populationSize - размер популяции 
	mutationPercent - вероятность мутации особи, double из [0, 1]
	percentToMate - процент особей, получающих возможность дать потомство, double из [0, 1]
	inputPath - путь к файлу с тестовыми данными
	idealPath - путь к файлу с идеальными результами для тестовых данных
Пример: Genetic 5000 0.1 0.25 res\input.tst res\ideal.tst (необходима сеть с 2 входными и одним выходным нейроном)

Для начала обучения по алгоритму Simulated Annealing используется команда Annealing
 -- Annealing startTemp stopTemp cycles inputPath idealPath
	startTemp - начальная температура отжига
	stopTemp - конечная температура отжига
	cycles - количество циклов понижения температуры в каждой итерации
	inputPath - путь к файлу с тестовыми данными
	idealPath - путь к файлу с идеальными результами для тестовых данных
Пример: Annealing 10 2 100 res\input.tst res\ideal.tst (необходима сеть с 2 входными и одним выходным нейроном)

Для просмотра результата выполнения следующей итерации выполняющейся тренировки используется команда Iterate
 -- Iterate

Просматривать итерации по порядку, одну за другой может не очень удобно. Посему была введена команда Offset, изменяющая шаг просмотра
 -- Offset iterationOffset
	iteratioOffset - количество пропускаемых при вызове iterate итераций
Пример: Offset 100 - если к моменту выполнения этой операции номер текущей итерации был равен 20, то следующий вызов Iterate приведет к 120 итерации

Чтобы остановить текущую тренировку можно использовать команду stop
 -- Stop

Чтобы просмотреть следующий шаг работы алгоритма Incremental Prune, необходимо использовать команду Prune
 -- Prune

Также имплементирован, но недоотлажен метод Selective Prune(неясные проблемы с выходом за границы матриц при попытке удаления столбца)